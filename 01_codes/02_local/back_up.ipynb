{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc69511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "tweet_features_2 = joblib.load(f\"../../02_data/tweet_features_2.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f0b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 200000, End: 300000\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = 100_000\n",
    "\n",
    "i = 2\n",
    "\n",
    "START = i * CHUNK_SIZE\n",
    "END = min(START + CHUNK_SIZE, len(tweet_features_2))\n",
    "\n",
    "print(f\"Start: {START}, End: {END}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594e70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding text: 100%|██████████| 98/98 [1:29:34<00:00, 54.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings 200000:300000\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "chunk = tweet_features_2.iloc[START:END].copy()\n",
    "\n",
    "embedder = SentenceEmbedder()\n",
    "chunk[\"embedding\"] = embedder.transform(chunk[\"text_normalized\"])[:, 0]\n",
    "\n",
    "joblib.dump(chunk, f\"../../02_data/tweet_features_3_{START}_{END}.joblib\")\n",
    "\n",
    "del chunk\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Saved embeddings {START}:{END}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c31224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweet_features_3_0_200000.joblib\n",
      "Loading tweet_features_3_200000_300000.joblib\n",
      "Loading tweet_features_3_300000_600000.joblib\n",
      "Loading tweet_features_3_600000_900000.joblib\n",
      "Loading tweet_features_3_900000_1048873.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "DATA_DIR = \"../../02_data\"\n",
    "\n",
    "pattern = re.compile(r\"tweet_features_3_(\\d+)_(\\d+)\\.joblib\")\n",
    "\n",
    "files = [\n",
    "    f for f in os.listdir(DATA_DIR)\n",
    "    if pattern.match(f)\n",
    "]\n",
    "\n",
    "files_sorted = sorted(\n",
    "    files,\n",
    "    key=lambda f: int(pattern.match(f).group(1))\n",
    ")\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for f in files_sorted:\n",
    "    path = os.path.join(DATA_DIR, f)\n",
    "    print(f\"Loading {f}\")\n",
    "    df = joblib.load(path)\n",
    "    dfs.append(df)\n",
    "\n",
    "tweet_features_all = pd.concat(dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3c263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../02_data/tweet_features_3.joblib']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joblib.dump(tweet_features_all, f\"../../02_data/tweet_features_3.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72454d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_counts = (\n",
    "    tweet_0.groupby(\"user_id\")\n",
    "    .size()\n",
    "    .reset_index(name=\"n_tweets\")\n",
    ")\n",
    "tweet_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b43ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_colwidth', None)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
