{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nAZMKnYVV-_R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urws6dvJk_be",
        "outputId": "390ac08a-52a4-452d-eaf4-e3edac48fd49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji vaderSentiment textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g5-1LJDlg9J",
        "outputId": "28677af8-9614-458e-eea4-b57f53703a02"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.12-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textstat) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2026.1.4)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textstat-0.7.12-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.6/176.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, emoji, vaderSentiment, textstat\n",
            "Successfully installed emoji-2.15.0 pyphen-0.17.2 textstat-0.7.12 vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggjqvfvakz1U",
        "outputId": "f9cde3e4-247b-4402-d988-ef70435927c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "u_QLvV2ikz1V"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import zlib\n",
        "import numpy as np\n",
        "import emoji\n",
        "\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import textstat\n",
        "from textblob import TextBlob\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "DIGIT_PATTERN = r\"\\d\"\n",
        "SPECIAL_CHAR_PATTERN = r\"[^A-Za-z0-9 ]\"\n",
        "USER_PATTERN = r\"(?<!\\w)@[A-Za-z0-9_]{1,15}\\b\"\n",
        "URL_PATTERN = r\"(https?://[^\\s]+|www\\.[^\\s]+)\"\n",
        "HASHTAG_PATTERN = r\"#\\w+\"\n",
        "CASHTAG_PATTERN = r\"\\$\\w+\"\n",
        "EMAIL_PATTERN = r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\"\n",
        "\n",
        "FUNCTION_WORDS = {\n",
        "  \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"if\", \"while\", \"with\", \"to\", \"of\", \"in\", \"on\",\n",
        "  \"for\", \"from\", \"by\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\"\n",
        "}\n",
        "\n",
        "def normalize_entities(text):\n",
        "  if not isinstance(text, str) or text.strip() == \"\":\n",
        "    return text\n",
        "\n",
        "  text = re.sub(EMAIL_PATTERN, \"<EMAIL>\", text)\n",
        "  text = re.sub(URL_PATTERN, \"<URL>\", text)\n",
        "  text = re.sub(USER_PATTERN, \"<USER>\", text)\n",
        "\n",
        "  return text\n",
        "\n",
        "def extract_text_features(text, is_tweet=False):\n",
        "  if not isinstance(text, str) or text.strip() == \"\":\n",
        "    return {\n",
        "        \"is_present\": False,\n",
        "        \"length\": None,\n",
        "        \"num_words\": None,\n",
        "        \"num_sentences\": None,\n",
        "        \"avg_sentence_length\": None,\n",
        "        \"avg_word_length\": None,\n",
        "        \"std_word_length\": None,\n",
        "        \"unique_word_ratio\": None,\n",
        "        \"guiraud_index\": None,\n",
        "        \"repetition_ratio\": None,\n",
        "        \"hapax_ratio\": None,\n",
        "        \"digit_ratio\": None,\n",
        "        \"uppercase_ratio\": None,\n",
        "        \"lowercase_ratio\": None,\n",
        "        \"special_char_ratio\": None,\n",
        "        \"punctuation_ratio\": None,\n",
        "        \"whitespace_ratio\": None,\n",
        "        \"emoji_count\": None,\n",
        "        \"emoji_ratio\": None,\n",
        "        \"mention_count\": None,\n",
        "        \"contains_mention\": False,\n",
        "        \"url_count\": None,\n",
        "        \"contains_url\": False,\n",
        "        \"hashtag_count\": None,\n",
        "        \"cashtag_count\": None,\n",
        "        \"email_count\": None,\n",
        "        \"contains_bot_word_or_hashtag\": False,\n",
        "        \"contains_ai_hashtag\": False,\n",
        "        \"sentiment\": None,\n",
        "        \"sentiment_abs\": None,\n",
        "        \"sentiment_neutrality\": None,\n",
        "        \"sentiment_subjectivity\": None,\n",
        "        \"flesch_reading_ease\": None,\n",
        "        \"flesch_kincaid_grade\": None,\n",
        "        \"avg_syllables_per_word\": None,\n",
        "        \"polysyllabic_word_ratio\": None,\n",
        "        \"char_entropy\": None,\n",
        "        \"word_entropy\": None,\n",
        "        \"compression_ratio\": None,\n",
        "        \"starts_with_emoji\": False,\n",
        "        \"ends_with_emoji\": False,\n",
        "        \"starts_with_url\": False,\n",
        "        \"ends_with_url\": False,\n",
        "        \"contains_pipe_or_bullet\": False,\n",
        "        \"contains_call_to_action\": False,\n",
        "        \"contains_ai_phrase\": False,\n",
        "        \"function_word_ratio\": None,\n",
        "        \"noun_ratio\": None,\n",
        "        \"verb_ratio\": None,\n",
        "        \"pronoun_ratio\": None,\n",
        "        \"adjective_ratio\": None,\n",
        "        \"contains_repeated_chars\": False,\n",
        "        \"is_retweet\": False,\n",
        "        \"is_quote\": False\n",
        "    }\n",
        "\n",
        "  text = text.strip()\n",
        "  text = normalize_entities(text)\n",
        "  char_len = len(text)\n",
        "\n",
        "  words = word_tokenize(text)\n",
        "  words_lower = [w.lower() for w in words if w.isalpha()]\n",
        "  num_words = len(words_lower)\n",
        "\n",
        "  sentences = sent_tokenize(text)\n",
        "  num_sentences = len(sentences)\n",
        "\n",
        "  # --- A: Presence & length\n",
        "  avg_sentence_length = num_words / num_sentences if num_sentences else None\n",
        "\n",
        "  # --- B: Lexical structure\n",
        "  word_lengths = [len(w) for w in words_lower]\n",
        "  avg_word_length = np.mean(word_lengths) if word_lengths else None\n",
        "  std_word_length = np.std(word_lengths) if word_lengths else None\n",
        "\n",
        "  word_counts = Counter(words_lower)\n",
        "  unique_word_ratio = len(word_counts) / num_words if num_words else None\n",
        "  guiraud_index = len(word_counts) / np.sqrt(num_words) if num_words else None\n",
        "  repetition_ratio = 1 - unique_word_ratio if unique_word_ratio is not None else None\n",
        "  hapax_ratio = sum(1 for w in word_counts if word_counts[w] == 1) / num_words if num_words else None\n",
        "\n",
        "  # --- C: Character composition\n",
        "  digits = len(re.findall(DIGIT_PATTERN, text))\n",
        "  letters = re.findall(r\"[A-Za-z]\", text)\n",
        "  uppercase = sum(1 for c in letters if c.isupper())\n",
        "  lowercase = sum(1 for c in letters if c.islower())\n",
        "  special_chars = len(re.findall(SPECIAL_CHAR_PATTERN, text))\n",
        "  punctuation = sum(1 for c in text if c in string.punctuation)\n",
        "  whitespaces = text.count(\" \")\n",
        "\n",
        "  emoji_count = sum(1 for c in text if c in emoji.EMOJI_DATA)\n",
        "\n",
        "  digit_ratio = digits / char_len\n",
        "  uppercase_ratio = uppercase / len(letters) if letters else None\n",
        "  lowercase_ratio = lowercase / len(letters) if letters else None\n",
        "  special_char_ratio = special_chars / char_len\n",
        "  punctuation_ratio = punctuation / char_len\n",
        "  whitespace_ratio = whitespaces / char_len\n",
        "  emoji_ratio = emoji_count / char_len\n",
        "\n",
        "  # --- D: Token usage\n",
        "  mention_count = text.count(\"<USER>\")\n",
        "  url_count = text.count(\"<URL>\")\n",
        "  hashtag_count = len(re.findall(HASHTAG_PATTERN, text))\n",
        "  cashtag_count = len(re.findall(CASHTAG_PATTERN, text))\n",
        "  email_count = text.count(\"<EMAIL>\")\n",
        "\n",
        "  # --- E: Semantic signals\n",
        "  contains_bot_word_or_hashtag = bool(re.search(r\"(?i)(\\bbot\\b|#\\w*bot\\b)\", text))\n",
        "  contains_ai_hashtag = bool(re.search(r\"(?i)\\b#ai\\b|#\\w+ai\\b\", text))\n",
        "\n",
        "  sentiment = analyzer.polarity_scores(text)[\"compound\"]\n",
        "  sentiment_abs = abs(sentiment)\n",
        "  sentiment_neutrality = 1 - sentiment_abs\n",
        "\n",
        "  blob = TextBlob(text)\n",
        "  sentiment_subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "  # --- F: Readability\n",
        "  flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
        "  flesch_kincaid_grade = textstat.flesch_kincaid_grade(text)\n",
        "  avg_syllables_per_word = textstat.avg_syllables_per_word(text)\n",
        "  polysyllabic_word_ratio = textstat.polysyllabcount(text) / num_words if num_words else None\n",
        "\n",
        "  # --- G: Entropy & compression\n",
        "  char_entropy = entropy(list(Counter(text).values()), base=2)\n",
        "  word_entropy = entropy(list(word_counts.values()), base=2) if word_counts else None\n",
        "  avg_word_repetition = np.mean(list(word_counts.values())) if word_counts else None\n",
        "  compression_ratio = len(zlib.compress(text.encode(\"utf-8\"))) / char_len\n",
        "\n",
        "  # --- H: Template indicators\n",
        "  starts_with_emoji = text[0] in emoji.EMOJI_DATA\n",
        "  ends_with_emoji = text[-1] in emoji.EMOJI_DATA\n",
        "  starts_with_url = text.startswith(\"<URL>\")\n",
        "  ends_with_url = text.endswith(\"<URL>\")\n",
        "  contains_pipe_or_bullet = bool(re.search(r\"\\s[|•]\\s\", text))\n",
        "  contains_call_to_action = bool(re.search(r\"(?i)\\b(follow|dm|click|join|subscribe|contact|call|buy|giveaway|free|win|retweet|apply)\\b\", text))\n",
        "  contains_ai_phrase = bool(re.search(r\"(?i)\\b(powered by AI|autogenerated|generated by AI|AI assistant)\\b\", text))\n",
        "\n",
        "  # --- I: Grammatical composition (self-reference & POS)\n",
        "  function_word_ratio = sum(w in FUNCTION_WORDS for w in words_lower) / num_words if num_words else None\n",
        "\n",
        "  pos_tags = pos_tag(words_lower)\n",
        "  pos_counts = Counter(tag for _, tag in pos_tags)\n",
        "\n",
        "  noun_ratio = sum(pos_counts[t] for t in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]) / num_words if num_words else None\n",
        "  verb_ratio = sum(pos_counts[t] for t in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]) / num_words if num_words else None\n",
        "  pronoun_ratio = sum(pos_counts[t] for t in [\"PRP\", \"PRP$\"]) / num_words if num_words else None\n",
        "  adjective_ratio = sum(pos_counts[t] for t in [\"JJ\", \"JJR\", \"JJS\"]) / num_words if num_words else None\n",
        "\n",
        "  # --- J: Noise & stylistic irregularities\n",
        "  contains_repeated_chars = bool(re.search(r'(.)\\1{2,}', text))\n",
        "\n",
        "  # --- K: Platform-specific discourse markers (tweets only)\n",
        "  if is_tweet:\n",
        "    is_retweet = bool(re.match(r'^RT\\s+<USER>', text))\n",
        "    is_quote = bool(re.match(r'^(QT|“|\")', text))\n",
        "  else:\n",
        "    is_retweet = None\n",
        "    is_quote = None\n",
        "\n",
        "  return {\n",
        "      \"is_present\": True,\n",
        "      \"length\": char_len,\n",
        "      \"num_words\": num_words,\n",
        "      \"num_sentences\": num_sentences,\n",
        "      \"avg_sentence_length\": avg_sentence_length,\n",
        "      \"avg_word_length\": avg_word_length,\n",
        "      \"std_word_length\": std_word_length,\n",
        "      \"unique_word_ratio\": unique_word_ratio,\n",
        "      \"guiraud_index\": guiraud_index,\n",
        "      \"repetition_ratio\": repetition_ratio,\n",
        "      \"hapax_ratio\": hapax_ratio,\n",
        "      \"digit_ratio\": digit_ratio,\n",
        "      \"uppercase_ratio\": uppercase_ratio,\n",
        "      \"lowercase_ratio\": lowercase_ratio,\n",
        "      \"special_char_ratio\": special_char_ratio,\n",
        "      \"punctuation_ratio\": punctuation_ratio,\n",
        "      \"whitespace_ratio\": whitespace_ratio,\n",
        "      \"emoji_count\": emoji_count,\n",
        "      \"emoji_ratio\": emoji_ratio,\n",
        "      \"mention_count\": mention_count,\n",
        "      \"contains_mention\": mention_count > 0,\n",
        "      \"url_count\": url_count,\n",
        "      \"contains_url\": url_count > 0,\n",
        "      \"hashtag_count\": hashtag_count,\n",
        "      \"cashtag_count\": cashtag_count,\n",
        "      \"email_count\": email_count,\n",
        "      \"contains_bot_word_or_hashtag\": contains_bot_word_or_hashtag,\n",
        "      \"contains_ai_hashtag\": contains_ai_hashtag,\n",
        "      \"sentiment\": sentiment,\n",
        "      \"sentiment_abs\": sentiment_abs,\n",
        "      \"sentiment_neutrality\": sentiment_neutrality,\n",
        "      \"sentiment_subjectivity\": sentiment_subjectivity,\n",
        "      \"flesch_reading_ease\": flesch_reading_ease,\n",
        "      \"flesch_kincaid_grade\": flesch_kincaid_grade,\n",
        "      \"avg_syllables_per_word\": avg_syllables_per_word,\n",
        "      \"polysyllabic_word_ratio\": polysyllabic_word_ratio,\n",
        "      \"char_entropy\": char_entropy,\n",
        "      \"word_entropy\": word_entropy,\n",
        "      \"avg_word_repetition\": avg_word_repetition,\n",
        "      \"compression_ratio\": compression_ratio,\n",
        "      \"starts_with_emoji\": starts_with_emoji,\n",
        "      \"ends_with_emoji\": ends_with_emoji,\n",
        "      \"starts_with_url\": starts_with_url,\n",
        "      \"ends_with_url\": ends_with_url,\n",
        "      \"contains_pipe_or_bullet\": contains_pipe_or_bullet,\n",
        "      \"contains_call_to_action\": contains_call_to_action,\n",
        "      \"contains_ai_phrase\": contains_ai_phrase,\n",
        "      \"function_word_ratio\": function_word_ratio,\n",
        "      \"noun_ratio\": noun_ratio,\n",
        "      \"verb_ratio\": verb_ratio,\n",
        "      \"pronoun_ratio\": pronoun_ratio,\n",
        "      \"adjective_ratio\": adjective_ratio,\n",
        "      \"contains_repeated_chars\": contains_repeated_chars,\n",
        "      \"is_retweet\": is_retweet,\n",
        "      \"is_quote\": is_quote\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AcnJYp63kz1W"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from emoji import demojize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "USER_PATTERN = r\"(?<!\\w)@[A-Za-z0-9_]{1,15}\\b\"\n",
        "URL_PATTERN = r\"(https?://[^\\s]+|www\\.[^\\s]+)\"\n",
        "EMAIL_PATTERN = r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\"\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "def normalize_entities(text):\n",
        "  if not isinstance(text, str) or text.strip() == \"\":\n",
        "    return text\n",
        "\n",
        "  text = re.sub(EMAIL_PATTERN, \"<EMAIL>\", text)\n",
        "  text = re.sub(URL_PATTERN, \"<URL>\", text)\n",
        "  text = re.sub(USER_PATTERN, \"<USER>\", text)\n",
        "\n",
        "  return text\n",
        "\n",
        "def normalize_token(token):\n",
        "    token = token.replace(\"’\", \"'\").replace(\"…\", \"...\")\n",
        "\n",
        "    if token == \"<USER>\":\n",
        "        return \"@USER\"\n",
        "    if token == \"<URL>\":\n",
        "        return \"HTTPURL\"\n",
        "    if len(token) == 1:\n",
        "        return demojize(token)\n",
        "\n",
        "    return token\n",
        "\n",
        "def normalize_text(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return \"\"\n",
        "\n",
        "    text = text.strip()\n",
        "    text = normalize_entities(text)\n",
        "\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    norm_tweet = \" \".join(normalize_token(t) for t in tokens)\n",
        "\n",
        "    # contractions\n",
        "    norm_tweet = (\n",
        "        norm_tweet.replace(\"cannot \", \"can not \")\n",
        "                  .replace(\" n't \", \" n't \")\n",
        "                  .replace(\"ca n't\", \"can't\")\n",
        "                  .replace(\"ai n't\", \"ain't\")\n",
        "    )\n",
        "\n",
        "    # verb contractions\n",
        "    norm_tweet = (\n",
        "        norm_tweet.replace(\" 'm \", \" 'm \")\n",
        "                  .replace(\" 're \", \" 're \")\n",
        "                  .replace(\" 's \", \" 's \")\n",
        "                  .replace(\" 'll \", \" 'll \")\n",
        "                  .replace(\" 'd \", \" 'd \")\n",
        "                  .replace(\" 've \", \" 've \")\n",
        "    )\n",
        "\n",
        "    # time expressions\n",
        "    norm_tweet = (\n",
        "        norm_tweet.replace(\" p . m .\", \" p.m.\")\n",
        "                  .replace(\" p . m \", \" p.m \")\n",
        "                  .replace(\" a . m .\", \" a.m.\")\n",
        "                  .replace(\" a . m \", \" a.m \")\n",
        "    )\n",
        "\n",
        "    return \" \".join(norm_tweet.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WXEic1b_kz1X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class SentenceEmbedder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, model_name=\"vinai/bertweet-base\"):\n",
        "      self.model_name = model_name\n",
        "      self.model = AutoModel.from_pretrained(self.model_name)\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "      self.model.to(device)\n",
        "      self.model.eval()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "    def transform(self, X):\n",
        "      if isinstance(X, pd.Series):\n",
        "        X = X.values\n",
        "\n",
        "      out = np.empty((len(X), 1), dtype=object)\n",
        "      batch_size = 1024\n",
        "\n",
        "      loader = DataLoader(\n",
        "          X,\n",
        "          batch_size=batch_size,\n",
        "          shuffle=False\n",
        "          )\n",
        "      embeddings = []\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Embedding text\"):\n",
        "          inputs = self.tokenizer(\n",
        "              batch,\n",
        "              padding=True,\n",
        "              truncation=True,\n",
        "              max_length=64,\n",
        "              return_tensors=\"pt\"\n",
        "              )\n",
        "          inputs = {k: v.to(device) for k, v in inputs.items()}  # move tensors to GPU\n",
        "\n",
        "          outputs = self.model(**inputs)\n",
        "          batch_embeddings = outputs.last_hidden_state[:, 0, :]  # (n_samples, 768)\n",
        "          embeddings.append(batch_embeddings.cpu())\n",
        "\n",
        "      embeddings = torch.cat(embeddings, dim=0)  # shape: (N, 768)\n",
        "      out[:, 0] = list(embeddings.numpy())  # each row is a 768-D array\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c5MFnqshVh3w"
      },
      "outputs": [],
      "source": [
        "import pyarrow.parquet as pq\n",
        "\n",
        "path = \"/content/drive/MyDrive/twibot-22/processed\"\n",
        "\n",
        "user_features = pd.read_parquet(f\"{path}/user_features.parquet\", engine='pyarrow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10mbjUiEKZ1i",
        "outputId": "528b84e5-b26b-4c0c-e993-c9cddcd70dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset contains 99967 rows and 51 columns.\n"
          ]
        }
      ],
      "source": [
        "n_rows, n_columns = user_features.shape\n",
        "print(f\"The dataset contains {n_rows} rows and {n_columns} columns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OBHcdGaHkz1Y"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = [\"description_length\", \"has_description\",\n",
        "                   \"has_bot_word_in_description\", \"ratio_digits_in_description\",\n",
        "                   \"ratio_special_chars_in_description\", \"description_sentiment\",\n",
        "                   \"cashtag_in_description_count\", \"hashtag_in_description_count\",\n",
        "                   \"mention_in_description_count\", \"url_in_description_count\"]\n",
        "\n",
        "user_features = user_features.drop(columns=columns_to_drop)\n",
        "\n",
        "user_features['label'] = user_features['label'].map({'human': 0, 'bot': 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5M470clkz1Y"
      },
      "outputs": [],
      "source": [
        "desc_feature_df = user_features[\"description\"].apply(extract_text_features).apply(pd.Series)\n",
        "desc_feature_df.rename(columns={c: f\"desc_{c}\" for c in desc_feature_df.columns}, inplace=True)\n",
        "\n",
        "user_features_1 = pd.concat([user_features, desc_feature_df], axis=1)\n",
        "user_features_1.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eygug0I3kz1Y"
      },
      "outputs": [],
      "source": [
        "user_features_1[\"description_normalized\"] = user_features_1[\"description\"].apply(normalize_text)\n",
        "\n",
        "embedder = SentenceEmbedder()\n",
        "user_features_1[\"desc_embedding\"] = embedder.transform(user_features_1[\"description_normalized\"])[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEyZ5GiJkz1Y"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(user_features_1, f\"{path}/user_features_1.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpwsRKHKkz1Y"
      },
      "outputs": [],
      "source": [
        "tweet_features = pd.read_parquet(f\"{path}/tweet_features.parquet\", engine='pyarrow')\n",
        "tweet_features.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q92tph39kz1Z"
      },
      "outputs": [],
      "source": [
        "n_rows, n_columns = tweet_features.shape\n",
        "print(f\"The dataset contains {n_rows} rows and {n_columns} columns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbJpXQcVkz1Z"
      },
      "outputs": [],
      "source": [
        "tweet_features['label'] = tweet_features['label'].map({'human': 0, 'bot': 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgH1UmD7kz1Z"
      },
      "outputs": [],
      "source": [
        "tweet_feature_df = tweet_features[\"text\"].apply(lambda x: extract_text_features(x, is_tweet=True)).apply(pd.Series)\n",
        "#tweet_feature_df.rename(columns={c: f\"tweet_{c}\" for c in tweet_feature_df.columns}, inplace=True)\n",
        "\n",
        "tweet_features_1 = pd.concat([tweet_features, tweet_feature_df], axis=1)\n",
        "tweet_features_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vreSvQLAkz1Z"
      },
      "outputs": [],
      "source": [
        "tweet_features_1[\"text_normalized\"] = tweet_features_1[\"text\"].apply(normalize_text)\n",
        "\n",
        "embedder = SentenceEmbedder()\n",
        "tweet_features_1[\"embedding\"] = embedder.transform(tweet_features_1[\"text_normalized\"])[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXo0s-Izkz1Z",
        "outputId": "1229ca84-de75-4d3e-8f2b-4dce689e5bc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['../../02_data/tweet_features_1.joblib']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(tweet_features_1, f\"{path}/tweet_features_1.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK-fRkI_7ftc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "thesis-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}